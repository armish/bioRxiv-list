{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import trange, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################\n",
    "# Copied/pasted from https://realpython.com/python-web-scraping-practical-introduction  #\n",
    "#########################################################################################\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the total number of pages to be scraped\n",
    "base_url = 'https://www.biorxiv.org/content/early/recent'\n",
    "raw_html = simple_get(base_url)\n",
    "html = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "page_count = int(html.select('li.pager-last > a')[0].text)\n",
    "print(page_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape all the pages (takes about 2 seconds per page)\n",
    "preprints = []\n",
    "\n",
    "for i in trange(page_count):\n",
    "    url = 'https://www.biorxiv.org/content/early/recent?page={}'.format(i)\n",
    "    \n",
    "    raw_html = simple_get(url)\n",
    "    html = BeautifulSoup(raw_html, 'html.parser')\n",
    "\n",
    "    for div in html.find_all(name='div', attrs={'class': 'highwire-list-wrapper'}):\n",
    "        pub_date_txt = div.find('h3').text\n",
    "        # February 5, 2019 -> 20190205\n",
    "        pub_date = datetime.strptime(pub_date_txt, '%B %d, %Y')\n",
    "\n",
    "        for li in div.select('div > ul > li'):\n",
    "            p_div = li.find('div', attrs={'class': 'highwire-article-citation'})\n",
    "            p_attrs = p_div.attrs\n",
    "            \n",
    "            # some articles don't have type annotation\n",
    "            p_type = p_div.select('div.highwire-cite-metadata > span.highwire-cite-metadata-overline')\n",
    "            if len(p_type) < 1:\n",
    "                p_type_str = ''\n",
    "            else:\n",
    "                p_type_str = p_type[0].text\n",
    "           \n",
    "            # really old articles have missing titles\n",
    "            p_title = p_attrs.get('title', None)\n",
    "            if p_title is None:\n",
    "                p_title = p_attrs.get('oldtitle', None)\n",
    "\n",
    "            preprint = {'node_id': p_attrs['data-node-nid'],\n",
    "                        'version_id': p_attrs['data-pisa'].split(';')[1],\n",
    "                        'master_id': p_attrs['data-pisa-master'].split(';')[1],\n",
    "                        'title': p_title,\n",
    "                        'preprint_type': p_type_str,\n",
    "                        'is_revision': not p_attrs['data-pisa'].endswith('v1'),\n",
    "                        'pub_date': pub_date_txt,\n",
    "                        'pub_date_year': pub_date.year,\n",
    "                        'pub_date_month': pub_date.month,\n",
    "                        'pub_date_day': pub_date.day}\n",
    "\n",
    "            preprints.append(preprint)\n",
    "        \n",
    "preprints_df = pd.DataFrame.from_dict(preprints)\n",
    "preprints_df.to_csv('preprints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total preprints scraped: {}'.format(len(preprints)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
